{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryansilvatrb/Sistema_de_Reconhecimento_Facial/blob/main/colab_webcam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzRKNe1iSlNW"
      },
      "source": [
        "# Google Colab: Access Webcam for Images and Video\n",
        "This notebook will go through how to access and run code on images and video taken using your webcam.  \n",
        "\n",
        "For this purpose of this tutorial we will be using OpenCV's Haar Cascade to do face detection on our Webcam image and video."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ====================================\n",
        "# CONFIGURAÇÕES\n",
        "# ====================================\n",
        "dataset_path = \"dataset\"\n",
        "pessoas = [\"Sheldon\", \"Penny\", \"Leonard\", \"Raj\", \"Amy\", \"Bernadette\"]\n",
        "img_size = 100\n",
        "\n",
        "# ====================================\n",
        "# PREPARAR DATASET\n",
        "# ====================================\n",
        "X, y = [], []\n",
        "\n",
        "for idx, pessoa in enumerate(pessoas):\n",
        "    folder = os.path.join(dataset_path, pessoa)\n",
        "    for file in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        resized = cv2.resize(gray, (img_size, img_size))\n",
        "\n",
        "        X.append(resized)\n",
        "        y.append(idx)\n",
        "\n",
        "X = np.array(X) / 255.0\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "y = to_categorical(y, num_classes=len(pessoas))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ====================================\n",
        "# DEFINIR MODELO CNN\n",
        "# ====================================\n",
        "model = Sequential([\n",
        "    Input(shape=(img_size, img_size, 1)),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(pessoas), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# ====================================\n",
        "# TREINAR MODELO\n",
        "# ====================================\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Força CPU caso GPU dê erro\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=4)\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Acurácia no teste: {acc*100:.2f}%\")\n",
        "\n",
        "# Salvar modelo\n",
        "model.save(\"modelo_reconhecimento.h5\")\n",
        "\n",
        "# ====================================\n",
        "# FUNÇÃO PARA DETECTAR E RECONHECER ROSTOS\n",
        "# ====================================\n",
        "def reconhecer_faces(imagem_path, modelo_path=\"modelo_reconhecimento.h5\"):\n",
        "    # Carregar modelo\n",
        "    model = load_model(modelo_path)\n",
        "\n",
        "    # Carregar classificador de rostos do OpenCV\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "    # Carregar imagem\n",
        "    img = cv2.imread(imagem_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detectar rostos\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face = gray[y:y+h, x:x+w]\n",
        "        face_resized = cv2.resize(face, (img_size, img_size)) / 255.0\n",
        "        face_resized = np.expand_dims(face_resized, axis=-1)\n",
        "        face_resized = np.expand_dims(face_resized, axis=0)\n",
        "\n",
        "        # Predição\n",
        "        pred = model.predict(face_resized, verbose=0)\n",
        "        idx = np.argmax(pred)\n",
        "        nome = pessoas[idx]\n",
        "        confianca = pred[0][idx]\n",
        "\n",
        "        # Desenhar na imagem\n",
        "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(img, f\"{nome} ({confianca:.2f})\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.8, (255, 0, 0), 2)\n",
        "\n",
        "    # Mostrar resultado\n",
        "    cv2.imshow(\"Resultado\", img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# ====================================\n",
        "# EXEMPLO DE USO\n",
        "# ====================================\n",
        "# Depois do treino, use assim para reconhecer em uma nova imagem:\n",
        "# reconhecer_faces(\"foto_teste.jpg\")\n"
      ],
      "metadata": {
        "id": "mASAFH27opov",
        "outputId": "bec203d7-6dfe-45b5-93e8-093ac474ac8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.1493 - loss: 2.7078 - val_accuracy: 0.0000e+00 - val_loss: 2.0695\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1493 - loss: 1.7989 - val_accuracy: 0.0000e+00 - val_loss: 1.9140\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1736 - loss: 1.7311 - val_accuracy: 0.0000e+00 - val_loss: 1.8594\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4479 - loss: 1.6570 - val_accuracy: 0.0000e+00 - val_loss: 1.8701\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6771 - loss: 1.5021 - val_accuracy: 0.0000e+00 - val_loss: 1.9044\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7083 - loss: 1.3122 - val_accuracy: 0.0000e+00 - val_loss: 2.0364\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6146 - loss: 1.3013 - val_accuracy: 0.0000e+00 - val_loss: 2.3220\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4722 - loss: 1.0990 - val_accuracy: 0.0000e+00 - val_loss: 2.6199\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6458 - loss: 0.8430 - val_accuracy: 0.0000e+00 - val_loss: 2.5356\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8507 - loss: 0.9827 - val_accuracy: 0.0000e+00 - val_loss: 2.4251\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 2.4251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia no teste: 0.00%\n"
          ]
        }
      ]
    }
  ]
}